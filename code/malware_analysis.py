#!/usr/bin/env python3
"""
Malware Image Analysis using Machine Learning Algorithms
Task 1: Gaussian Process Regression/Classification, Bayesian Networks, and LDA

This script implements:
1. Gaussian Process Regression and Classification for malware detection
2. Bayesian Network for malware family relationships
3. Latent Dirichlet Allocation for topic modeling of malware features
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings('ignore')

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class MalwareImageAnalyzer:
    def __init__(self, data_path="../data/malimg_paper_dataset_imgs"):
        self.data_path = data_path
        self.results_path = "../results"
        self.viz_path = "../visualizations"
        
        # Create directories if they don't exist
        os.makedirs(self.results_path, exist_ok=True)
        os.makedirs(self.viz_path, exist_ok=True)
        
        self.data = None
        self.features = None
        self.labels = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
    def load_and_preprocess_data(self, max_samples_per_class=100):
        """Load malware images and extract features"""
        print("Loading and preprocessing malware images...")
        
        features_list = []
        labels_list = []
        
        # Get all malware families
        malware_families = [d for d in os.listdir(self.data_path) 
                          if os.path.isdir(os.path.join(self.data_path, d))]
        
        for family in malware_families:
            family_path = os.path.join(self.data_path, family)
            image_files = [f for f in os.listdir(family_path) if f.endswith('.png')]
            
            # Limit samples per class to avoid memory issues
            image_files = image_files[:max_samples_per_class]
            
            for img_file in image_files:
                img_path = os.path.join(family_path, img_file)
                try:
                    # Load and resize image
                    img = Image.open(img_path).convert('L')  # Convert to grayscale
                    img = img.resize((64, 64))  # Resize to 64x64
                    img_array = np.array(img)
                    
                    # Extract features
                    features = self.extract_image_features(img_array)
                    
                    features_list.append(features)
                    labels_list.append(family)
                    
                except Exception as e:
                    print(f"Error processing {img_path}: {e}")
                    continue
        
        self.features = np.array(features_list)
        self.labels = np.array(labels_list)
        
        print(f"Loaded {len(self.features)} samples with {len(np.unique(self.labels))} classes")
        print(f"Feature dimension: {self.features.shape[1]}")
        
        return self.features, self.labels
    
    def extract_image_features(self, img_array):
        """Extract features from image array"""
        features = []
        
        # 1. Basic statistics
        features.extend([
            np.mean(img_array),
            np.std(img_array),
            np.median(img_array),
            np.min(img_array),
            np.max(img_array),
            np.percentile(img_array, 25),
            np.percentile(img_array, 75)
        ])
        
        # 2. Histogram features
        hist, _ = np.histogram(img_array, bins=10, range=(0, 255))
        features.extend(hist)
        
        # 3. Texture features (GLCM-like)
        # Calculate gradients
        grad_x = cv2.Sobel(img_array, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(img_array, cv2.CV_64F, 0, 1, ksize=3)
        
        features.extend([
            np.mean(np.abs(grad_x)),
            np.mean(np.abs(grad_y)),
            np.std(grad_x),
            np.std(grad_y)
        ])
        
        # 4. Edge density
        edges = cv2.Canny(img_array.astype(np.uint8), 50, 150)
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        features.append(edge_density)
        
        # 5. Local Binary Pattern (simplified)
        lbp_features = self.simple_lbp(img_array)
        features.extend(lbp_features)
        
        return np.array(features)
    
    def simple_lbp(self, img_array, radius=1):
        """Simplified Local Binary Pattern"""
        height, width = img_array.shape
        lbp = np.zeros((height, width), dtype=np.uint8)
        
        for i in range(radius, height - radius):
            for j in range(radius, width - radius):
                center = img_array[i, j]
                code = 0
                
                # Check 8 neighbors
                neighbors = [
                    img_array[i-1, j-1], img_array[i-1, j], img_array[i-1, j+1],
                    img_array[i, j+1], img_array[i+1, j+1], img_array[i+1, j],
                    img_array[i+1, j-1], img_array[i, j-1]
                ]
                
                for k, neighbor in enumerate(neighbors):
                    if neighbor >= center:
                        code |= (1 << k)
                
                lbp[i, j] = code
        
        # Calculate histogram
        hist, _ = np.histogram(lbp, bins=8, range=(0, 256))
        return hist
    
    def gaussian_process_regression(self):
        """Implement Gaussian Process Regression"""
        print("\n=== Gaussian Process Regression ===")
        
        # Use first 4 features as input variables
        X = self.features[:, :4]  # 4 input variables
        y = self.features[:, 4]   # 5th feature as output variable
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Define kernel
        kernel = C(1.0, (1e-3, 1e3)) * RBF([1.0] * X_train_scaled.shape[1], (1e-2, 1e2))
        
        # Train Gaussian Process Regressor
        gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=10)
        gpr.fit(X_train_scaled, y_train)
        
        # Predictions
        y_pred, y_std = gpr.predict(X_test_scaled, return_std=True)
        
        # Calculate metrics
        mse = np.mean((y_test - y_pred) ** 2)
        rmse = np.sqrt(mse)
        r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)
        
        print(f"Mean Squared Error: {mse:.4f}")
        print(f"Root Mean Squared Error: {rmse:.4f}")
        print(f"RÂ² Score: {r2:.4f}")
        
        # Save results
        results = {
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'predictions': y_pred,
            'actual': y_test,
            'std': y_std
        }
        
        np.save(os.path.join(self.results_path, 'gpr_results.npy'), results)
        
        # Visualization
        self.plot_gpr_results(y_test, y_pred, y_std)
        
        return gpr, results
    
    def gaussian_process_classification(self):
        """Implement Gaussian Process Classification"""
        print("\n=== Gaussian Process Classification ===")
        
        # Use first 4 features as input variables
        X = self.features[:, :4]
        
        # Create binary classification by thresholding the 5th feature
        threshold = np.median(self.features[:, 4])
        y_binary = (self.features[:, 4] > threshold).astype(int)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_binary, test_size=0.2, random_state=42, stratify=y_binary
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Define kernel
        kernel = C(1.0, (1e-3, 1e3)) * RBF([1.0] * X_train_scaled.shape[1], (1e-2, 1e2))
        
        # Train Gaussian Process Classifier
        gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)
        gpc.fit(X_train_scaled, y_train)
        
        # Predictions
        y_pred = gpc.predict(X_test_scaled)
        y_prob = gpc.predict_proba(X_test_scaled)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"Accuracy: {accuracy:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        # Save results
        results = {
            'accuracy': accuracy,
            'predictions': y_pred,
            'probabilities': y_prob,
            'actual': y_test
        }
        
        np.save(os.path.join(self.results_path, 'gpc_results.npy'), results)
        
        # Visualization
        self.plot_gpc_results(y_test, y_pred, y_prob)
        
        return gpc, results
    
    def bayesian_network_analysis(self):
        """Implement Bayesian Network Analysis"""
        print("\n=== Bayesian Network Analysis ===")
        
        # Select 8 features for Bayesian Network
        feature_names = [
            'mean_intensity', 'std_intensity', 'median_intensity', 'min_intensity',
            'max_intensity', 'q25_intensity', 'q75_intensity', 'edge_density'
        ]
        
        # Create feature matrix with 8 variables
        X_bn = self.features[:, :8]
        
        # Discretize continuous variables for Bayesian Network
        X_discrete = self.discretize_features(X_bn)
        
        # Create correlation matrix
        correlation_matrix = np.corrcoef(X_bn.T)
        
        # Save correlation matrix
        np.save(os.path.join(self.results_path, 'bn_correlation.npy'), correlation_matrix)
        
        # Create and save feature dataframe
        df_bn = pd.DataFrame(X_bn, columns=feature_names)
        df_bn['malware_family'] = self.labels
        df_bn.to_csv(os.path.join(self.results_path, 'bn_features.csv'), index=False)
        
        # Visualization
        self.plot_bayesian_network(correlation_matrix, feature_names)
        
        # Calculate conditional probabilities (simplified)
        conditional_probs = self.calculate_conditional_probabilities(X_discrete, feature_names)
        
        results = {
            'correlation_matrix': correlation_matrix,
            'feature_names': feature_names,
            'conditional_probabilities': conditional_probs,
            'discrete_features': X_discrete
        }
        
        np.save(os.path.join(self.results_path, 'bn_results.npy'), results)
        
        return results
    
    def discretize_features(self, X, n_bins=5):
        """Discretize continuous features for Bayesian Network"""
        X_discrete = np.zeros_like(X, dtype=int)
        
        for i in range(X.shape[1]):
            bins = np.linspace(X[:, i].min(), X[:, i].max(), n_bins + 1)
            X_discrete[:, i] = np.digitize(X[:, i], bins) - 1
        
        return X_discrete
    
    def calculate_conditional_probabilities(self, X_discrete, feature_names):
        """Calculate conditional probabilities for Bayesian Network"""
        n_features = X_discrete.shape[1]
        conditional_probs = {}
        
        for i in range(n_features):
            for j in range(n_features):
                if i != j:
                    # Calculate P(feature_i | feature_j)
                    unique_j = np.unique(X_discrete[:, j])
                    probs = {}
                    
                    for val_j in unique_j:
                        mask = X_discrete[:, j] == val_j
                        if np.sum(mask) > 0:
                            unique_i = np.unique(X_discrete[mask, i])
                            probs[val_j] = {val_i: np.mean(X_discrete[mask, i] == val_i) 
                                          for val_i in unique_i}
                    
                    conditional_probs[f"{feature_names[i]}_given_{feature_names[j]}"] = probs
        
        return conditional_probs
    
    def latent_dirichlet_allocation(self):
        """Implement Latent Dirichlet Allocation for topic modeling"""
        print("\n=== Latent Dirichlet Allocation ===")
        
        # Use all features as "documents"
        X_lda = self.features
        
        # Apply PCA for dimensionality reduction
        pca = PCA(n_components=min(50, X_lda.shape[1]))
        X_pca = pca.fit_transform(X_lda)
        
        # Normalize features and ensure non-negative values for LDA
        X_normalized = (X_pca - X_pca.min(axis=0)) / (X_pca.max(axis=0) - X_pca.min(axis=0))
        
        # Apply LDA using sklearn
        from sklearn.decomposition import LatentDirichletAllocation
        
        n_topics = 10
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            learning_method='batch',
            max_iter=100
        )
        
        # Fit LDA
        topic_distributions = lda.fit_transform(X_normalized)
        
        # Get topic-word distributions
        topic_word_distributions = lda.components_
        
        # Calculate perplexity
        perplexity = lda.perplexity(X_normalized)
        
        print(f"Number of topics: {n_topics}")
        print(f"Perplexity: {perplexity:.4f}")
        
        # Analyze topic distributions per malware family
        family_topic_analysis = {}
        unique_families = np.unique(self.labels)
        
        for family in unique_families:
            mask = self.labels == family
            family_topics = topic_distributions[mask]
            family_topic_analysis[family] = {
                'mean_distribution': np.mean(family_topics, axis=0),
                'std_distribution': np.std(family_topics, axis=0),
                'count': np.sum(mask)
            }
        
        # Save results
        results = {
            'topic_distributions': topic_distributions,
            'topic_word_distributions': topic_word_distributions,
            'family_topic_analysis': family_topic_analysis,
            'perplexity': perplexity,
            'n_topics': n_topics,
            'pca_components': pca.components_,
            'explained_variance_ratio': pca.explained_variance_ratio_
        }
        
        np.save(os.path.join(self.results_path, 'lda_results.npy'), results)
        
        # Visualization
        self.plot_lda_results(topic_distributions, family_topic_analysis, n_topics)
        
        return lda, results
    
    def plot_gpr_results(self, y_test, y_pred, y_std):
        """Plot Gaussian Process Regression results"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Actual vs Predicted
        axes[0, 0].scatter(y_test, y_pred, alpha=0.6)
        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Values')
        axes[0, 0].set_ylabel('Predicted Values')
        axes[0, 0].set_title('GPR: Actual vs Predicted')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Residuals
        residuals = y_test - y_pred
        axes[0, 1].scatter(y_pred, residuals, alpha=0.6)
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('Predicted Values')
        axes[0, 1].set_ylabel('Residuals')
        axes[0, 1].set_title('GPR: Residuals Plot')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Prediction uncertainty
        axes[1, 0].scatter(y_test, y_std, alpha=0.6)
        axes[1, 0].set_xlabel('Actual Values')
        axes[1, 0].set_ylabel('Prediction Standard Deviation')
        axes[1, 0].set_title('GPR: Prediction Uncertainty')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Residuals histogram
        axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Residuals')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('GPR: Residuals Distribution')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.viz_path, 'gpr_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_gpc_results(self, y_test, y_pred, y_prob):
        """Plot Gaussian Process Classification results"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('GPC: Confusion Matrix')
        axes[0, 0].set_xlabel('Predicted')
        axes[0, 0].set_ylabel('Actual')
        
        # Probability distribution
        axes[0, 1].hist(y_prob[:, 1], bins=30, alpha=0.7, label='Class 1 Probability')
        axes[0, 1].set_xlabel('Probability')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('GPC: Class 1 Probability Distribution')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # ROC-like plot (simplified)
        thresholds = np.linspace(0, 1, 100)
        accuracies = []
        for threshold in thresholds:
            pred_thresh = (y_prob[:, 1] > threshold).astype(int)
            accuracies.append(accuracy_score(y_test, pred_thresh))
        
        axes[1, 0].plot(thresholds, accuracies)
        axes[1, 0].set_xlabel('Threshold')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].set_title('GPC: Accuracy vs Threshold')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Prediction confidence
        confidence = np.max(y_prob, axis=1)
        axes[1, 1].hist(confidence, bins=30, alpha=0.7)
        axes[1, 1].set_xlabel('Prediction Confidence')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('GPC: Prediction Confidence Distribution')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.viz_path, 'gpc_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_bayesian_network(self, correlation_matrix, feature_names):
        """Plot Bayesian Network correlation matrix"""
        plt.figure(figsize=(12, 10))
        
        # Create correlation heatmap
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5, 
                   xticklabels=feature_names, yticklabels=feature_names)
        
        plt.title('Bayesian Network: Feature Correlation Matrix')
        plt.tight_layout()
        plt.savefig(os.path.join(self.viz_path, 'bayesian_network_correlation.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # Feature distribution plots
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()
        
        for i, feature in enumerate(feature_names):
            axes[i].hist(self.features[:, i], bins=30, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'Distribution: {feature}')
            axes[i].set_xlabel('Value')
            axes[i].set_ylabel('Frequency')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.viz_path, 'bayesian_network_distributions.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_lda_results(self, topic_distributions, family_topic_analysis, n_topics):
        """Plot LDA results"""
        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        
        # Topic distribution heatmap
        family_names = list(family_topic_analysis.keys())
        topic_means = np.array([family_topic_analysis[family]['mean_distribution'] 
                              for family in family_names])
        
        sns.heatmap(topic_means, annot=True, cmap='viridis', 
                   xticklabels=[f'Topic {i+1}' for i in range(n_topics)],
                   yticklabels=family_names, ax=axes[0, 0])
        axes[0, 0].set_title('LDA: Average Topic Distribution by Malware Family')
        axes[0, 0].set_xlabel('Topics')
        axes[0, 0].set_ylabel('Malware Families')
        
        # Topic distribution across all samples
        axes[0, 1].boxplot([topic_distributions[:, i] for i in range(n_topics)])
        axes[0, 1].set_xlabel('Topics')
        axes[0, 1].set_ylabel('Topic Distribution')
        axes[0, 1].set_title('LDA: Topic Distribution Across All Samples')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Perplexity vs number of topics (if we had multiple runs)
        axes[1, 0].bar(range(1, n_topics + 1), 
                      np.mean(topic_distributions, axis=0))
        axes[1, 0].set_xlabel('Topics')
        axes[1, 0].set_ylabel('Average Topic Weight')
        axes[1, 0].set_title('LDA: Average Topic Weights')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Family counts
        family_counts = [family_topic_analysis[family]['count'] for family in family_names]
        axes[1, 1].barh(family_names, family_counts)
        axes[1, 1].set_xlabel('Number of Samples')
        axes[1, 1].set_title('LDA: Sample Count by Malware Family')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.viz_path, 'lda_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def run_complete_analysis(self):
        """Run complete analysis pipeline"""
        print("Starting Malware Image Analysis...")
        
        # Load and preprocess data
        self.load_and_preprocess_data(max_samples_per_class=50)
        
        # Run all analyses
        gpr_results = self.gaussian_process_regression()
        gpc_results = self.gaussian_process_classification()
        bn_results = self.bayesian_network_analysis()
        lda_results = self.latent_dirichlet_allocation()
        
        # Generate summary report
        self.generate_summary_report(gpr_results, gpc_results, bn_results, lda_results)
        
        print("\nAnalysis complete! Results saved in ../results/")
        print("Visualizations saved in ../visualizations/")
        
        return gpr_results, gpc_results, bn_results, lda_results
    
    def generate_summary_report(self, gpr_results, gpc_results, bn_results, lda_results):
        """Generate a summary report of all analyses"""
        report = []
        report.append("=" * 60)
        report.append("MALWARE IMAGE ANALYSIS - SUMMARY REPORT")
        report.append("=" * 60)
        report.append("")
        
        # GPR Results
        report.append("1. GAUSSIAN PROCESS REGRESSION")
        report.append("-" * 30)
        report.append(f"Mean Squared Error: {gpr_results[1]['mse']:.4f}")
        report.append(f"Root Mean Squared Error: {gpr_results[1]['rmse']:.4f}")
        report.append(f"RÂ² Score: {gpr_results[1]['r2']:.4f}")
        report.append("")
        
        # GPC Results
        report.append("2. GAUSSIAN PROCESS CLASSIFICATION")
        report.append("-" * 35)
        report.append(f"Accuracy: {gpc_results[1]['accuracy']:.4f}")
        report.append("")
        
        # Bayesian Network Results
        report.append("3. BAYESIAN NETWORK ANALYSIS")
        report.append("-" * 25)
        report.append(f"Number of variables: {len(bn_results['feature_names'])}")
        report.append(f"Variables: {', '.join(bn_results['feature_names'])}")
        report.append("")
        
        # LDA Results
        report.append("4. LATENT DIRICHLET ALLOCATION")
        report.append("-" * 30)
        report.append(f"Number of topics: {lda_results[1]['n_topics']}")
        report.append(f"Perplexity: {lda_results[1]['perplexity']:.4f}")
        report.append(f"Number of malware families: {len(lda_results[1]['family_topic_analysis'])}")
        report.append("")
        
        # Dataset Information
        report.append("5. DATASET INFORMATION")
        report.append("-" * 20)
        report.append(f"Total samples: {len(self.features)}")
        report.append(f"Feature dimension: {self.features.shape[1]}")
        report.append(f"Number of classes: {len(np.unique(self.labels))}")
        report.append("")
        
        report.append("=" * 60)
        
        # Save report
        with open(os.path.join(self.results_path, 'summary_report.txt'), 'w') as f:
            f.write('\n'.join(report))
        
        print('\n'.join(report))


if __name__ == "__main__":
    # Initialize analyzer
    analyzer = MalwareImageAnalyzer()
    
    # Run complete analysis
    analyzer.run_complete_analysis() 